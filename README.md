## 修改日志

### 2022/1/12

完成了学习率的自适应，采用的是 AdaGrad 方法，在一定条件下，可以不提供初始学习率，不断学习得到学习率。仍然存在的问题有，全局学习率的设置，以及衰减速率的设置，这些都导致了学习率并不容易自己进行调节。但是在调节过程中通过分析学习率的变化大概知道学习率的范围，也有利于我们去设置学习率。也有可能是我的学习率自适应算法编写存在问题，导致效果并没有达到预期。

同时今天在训练过程中发现了，当我在某种神经网络结构下训练不出模型时，扩大模型输出层的数量，反而使模型得到了成功的训练，但是无法理解这个现象。

![image-20220112103224310](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112103224310.png)

神经网络作为非线性模型依然没有很好的区分开分类集 GMM8 的上难以区分的两类。

![image-20220112134209204](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112134209204.png)

alpha=[0.01,0.001,0.01,0.01] 6-8-4

![image-20220112134544833](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112134544833.png)

alpha=[0.001,0.0015,0.01,0.01] 6-8

一个很有趣的错误：如果对于 GMM4 样本集采用结构为 6-16-4 的神经网络，训练失败，但若将网络改为 6-16-5，就可以训练得到非常棒的结果，原因未知。

学习率自适应设计失败，接下来将尝试在训练过程中进行进行自适应。

给定初始学习率，后进行学习率自适应反而加剧了震荡，没有达到理想的结果，而且与全局学习率存在很大的关系。

![image-20220112152259748](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112152259748.png)

学习率自适应：

e=1e-6

![image-20220112160855226](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112160855226.png)

![image-20220112182003970](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112182003970.png)

![image-20220112182023091](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112182023091.png)

![image-20220112182039968](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112182039968.png)

![image-20220112195501941](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112195501941.png)

没有采用自适应学习率统一设置

![image-20220112195819865](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112195819865.png)

采用自适应

![image-20220112195855638](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112195855638.png)

分别设置

![image-20220112200647969](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220112200647969.png)

将错误的学习率纠正

错误学习率纠正：e =1e-5,rou = 0.1



### 2022/1/8



开始编译前向神经网络。一开始尝试将常数值写进神经元，即给神经元加入恒为1的常量，但是在进行权重更新时，十分不便，遂放弃，更改为在激活函数中加入常数值，目前还未完成。

### 2022/1/10

一月九号将神经网络的预测和训练过程都写完了，但是预测结果十分糟糕，在测试集上的正确率也只有0.1左右，可以认为是训练失败，通过分界面可以看到在训练集上的效果也完全没有分开。

#### 原因包括以下几点：

1、一开始想要将激活函数中的 theta 和 gama 替换成在输入层和隐藏层的常数神经元，但是在权重更新过程中就需要不断地增加删减矩阵的某一列，十分麻烦，于是放弃这种方式，改用显现的将 theta 和 gama 求出来。

2、没有对数据进行归一化，导致 softmax 函数和 sigmoid 函数发生溢出问题。

3、更新权重的函数在传递过程中出现顺序错误。

4、**学习率设置**

在一开始的训练过程中，学习率设置的是0.1，出现的结果就是完全训练不出结果，将学习率改为0.01后学习效果瞬间变好，具体原因还没有想明白。

5、**网络结构设计**

网络结构一开始采用的是 2-4-4，后来采用 2-3-4 发现效果能够更好一些，具体原因未知。

6、样本特征太少，只有两个。

#### 现在要解决的问题包括：

1、怎么设计网络结构？

2、怎么调整学习率？

3、对于特征很少的情况怎么解决？

### 2022/1/11

对于 GMM6 数据集，采用隐藏层数量为64，学习率为0.001后，得到目前最好的效果。

对于三分类问题隐藏层设置不宜过多，调整为 8 个后分类效果明显得到改善

![image-20220111203656912](C:\Users\syr11\AppData\Roaming\Typora\typora-user-images\image-20220111203656912.png)



GMM 中出现溢出现象

numpy.dot 在处理一维矩阵的叉乘时存在问题

神经网络升维以后：![image-20220111224839196](D:\桑养\杂项\ANN+128+3000+0.001五倍交叉验证GMM6)

继续优化神经网络，编写实现混合高斯分布的贝叶斯模型，实现五折交叉验证，实现了维度的提高。

对于样本集 GMM8 中存在的样本空间分布紧密的现象，线性模型不具有良好的分类效果，正确率都位于0.875以下，可以认为对于右上角的两类分类完全失败。但这也正是线性分类器无法解决的 XOR 问题。

多层神经网络作为非线性模型，本应该在GMM8上具有较好的分类能力，但是我的实现并没有得到这样的结果，可能是代码的编写存在问题，或者样本的特征太少导致的.
